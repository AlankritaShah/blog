---
layout: post
title: "Ich fahre, du bist schuld"
tagline: "Warum ich mich zuletzt über Tesla geärgert habe"
description: ""
category: []
---
{% include JB/setup %}

In den letzten Wochen habe ich mich mehrfach zu [Tweets](https://twitter.com/nlohmann) hinreißen lassen, in denen ich über Tesla hergezogen habe. Wenig überraschend konnte ich meinen Ärger nicht sinnvoll in 140 Zeichen unterbringen. Dieses andauernde Meckern nervt jeden, sodass ich hier versuche, mein Problem ausführlicher zu schildern.

## Vorgeschichte

Was war passiert? Am 7. Mai 2016 starb ein Fahrer eines [Tesla Model S](https://en.wikipedia.org/wiki/Tesla_Model_S). Sowohl Fahrer als auch der Autopilot übersahen einen den Highway überquerenden Sattelzug, sodass der Tesla ungebremst unter den Aufleger rutschte und der Fahrer starb. Der Unfall ist offenbar das Ergebnis von vielen Fehlern und unglücklichen Umständen:

- Zum einen hat der Fahrer zuvor mehrere [YouTube-Videos](http://sfist.com/2016/07/03/driver_in_fatal_tesla_crash_posted.php) veröffentlicht, in denen er -- entgegen der Vorgaben von Tesla -- die Hände vom Lenkrad genommen hat und dem Autopiloten die komplette Kontrolle über das Fahrzeug gegeben hat. Zusätzlich gibt es [Gerüchte](http://www.telegraph.co.uk/news/2016/07/01/tesla-autopilot-crash-driver-was-watching-harry-potter-movie-wit/), der Fahrer hätte während des Unfalls auf einem tragbaren DVD-Player einen Harry-Potter-Film geschaut. In jedem Fall ist nicht auszuschließen, dass der Fahrer zum Zeitpunkt des Unfalls das Fahrzeug nicht kontrolliert hat. Weiterhin kann es sein, dass mit überhöhter Geschwindigkeit gefahren wurde.

- Zum anderen versagte der (eingeschaltete) Autopilot, der den (weißen) Aufleger offenbar vor dem hellen Himmel übersehen bzw. ihn mit einem Straßenschild verwechselt hat und daher keine Notbremsung bzw. Tempoanpassung eingeleitet hat. Zusätzlich hat der Tesla den Aufprall selbst offenbar nicht "mitbekommen" und ist zunächst weiter gefahren, bevor der Wagen von der Straße abgekommen und nach einem weiteren Aufprall zum Stehen kam.

Ohne diese widrigen Umstände wäre es sicher nicht zu dem Todesfall gekommen, sei es durch rechtzeitiges Eingreifen durch Fahrer oder Autopilot oder durch einen günstigeren Einschlagwinkel (z.B. gegen das Heck des Sattelzuges), der die Überlebenswahrscheinlichkeit des Fahrers stark erhöht hätte.

## Der Tesla-Autopilot

Das von Tesla als "[Autopilot](https://www.teslamotors.com/presskit/autopilot)" vermarktete System vereint eine Reihe von modernen Fahrassistenzsystemen:

- 360°-Abstandsmessung: Eine Kamera, Radar und viele Ultraschallsensoren überwachen die Abstände zur Umgebung des Fahrzeuges. Im Falle eines Hindernisses warnt das Fahrzeug den Fahrer bzw. bremst selbstständig ab.

- Verkehrszeichenerkennung: Geschwindigkeitsbegrenzungen oder Baustellenschilder ergänzen die Daten aus dem Navigationssystem. Offenbar werden Ampeln oder Sperrungen jedoch nicht erkannt.

- Spurhalteassistent: Durch Erkennen von Fahrstreifen kann das Fahrzeug selbstständig die Fahrspur halten und auch in Kurven selbstständig lenken.

- Adaptive Cruise Control (ACC): Das Fahrzeug hält selbstständig eine vorgegebene Geschwindigkeit und einen Sicherheitsabstand zu vorausfahrenden Fahrzeugen -- Bremsen und Beschleunigen inklusive.

- Spurwechselassistent: Zusätzlich zur ACC kann durch betätigen des Blinkers ein automatischer Spurwechsel eingeleitet werden, falls das Fahrzeug entsprechend eine ausreichende "Lücke" auf der Überholspur erkennt.

Ein Großteil dieser Assistenten sind auch in Oberklassefahrzeugen anderer Hersteller verbaut, und jedes einzelne ist definitiv ein Sicherheitsgewinn.

## Mein Problem

Die Kollision in Florida war der erste tödliche Unfall mit einem von einem Computer gesteuerten Fahrzeug. Es gab sicherlich schon unzählige Unfälle die Fahrassistenzssysteme nicht verhindern konnten. Diese Systeme wurden jedoch nicht als "Autopilot" oder "Autosteer" vermarket. Das System von Tesla funktioniert gut. So gut, dass es wie autonomes Fahren wahrgenommen wird. Marco Arment [bringt es auf den Punkt](https://marco.org/2016/07/06/tesla-autopilot):

> It’s automated enough that people *will* stop paying attention, but it’s not good enough that they *can*.

Mich stört dieser Konflikt zwischen öffentlichem Auftreten von Tesla (gerade vor dem Unfall) auf der einen, sowie der Realität auf der anderen Seite, die im Handbuch und Bestätigungsdialogen korrekt an die Grenzen der Systeme und an die Pflicht, stets die Hände am Lenkrad zu halten und eingreifbereit zu bleiben, erinnert. Medienberichte feierten die einzigartigen Innovationen und sorgten für eine überhöhte Erwartungshaltung. Es gibt unzählige Videos von Tesla-Fahrern, die lange Strecken ohne Hände am Lenkrad fahren. Tesla musste davon wissen und hätte ihr System besser **vor solchen Missbräuchen schützen** müssen.

Bei der Entwicklung von autonomen Fahrzeugen wurden bisher ausgiebig konstruierte Situationen diskutiert, in denen das Fahrzeug algorithmisch entscheiden soll, ob es seine Insassen durch einen Crash in eine Mauer tötet oder sie rettet, indem es eine Kindergartenklasse überfährt. Auf dem Weg zu solch philosophischen Fragen müssen jedoch schon bei teilautonomen Fahren viel banalerer Probleme angegangen werden, und wer Vorreiter in diesem Bereich sein möchte, muss auch diese Probleme verantwortungsvoll lösen.

Die aktuellen Systeme sind vor allem noch weit davon entfernt, wirklich komplexe Szenarien zu meistern: Stadtverkehr, Kreuzungen oder Kreisverkehre sind sind für Autopiloten noch ferne Zukunftsmusik. Stattdessen kann bei überschaubaren und monotonen Strecken gepunktet werden. Doch wenn bisher ein gelangweilter Fahrer im Stop-and-Go-Verkehr ein Unfallrisiko darstellte -- wie sinnvoll ist dann ein Autopilot, der diese Aufgabe zwar überdurchschnittlich gut meistert, jedoch noch immer einen aufmerksamen Fahrer am Steuer voraussetzt und ihn **nie aus der Verantwortung entlässt**, obwohl er mehr und mehr zum Zuschauer degradiert wird.

Hier reicht eine bemerkenswerte Ingeneurleistung nicht aus, und die auftretenden Probleme lassen sich **nicht per AGB und Bestätigungsdialog** aus der Welt schaffen.


## Teslas Reaktion

Der Unfall passierte am 7. Mai 2016. Tesla gab am 30. Juni 2016 eine [Pressemitteilung](https://www.teslamotors.com/blog/tragic-loss) heraus. Vordergründig ging es um eine durch die Sicherheitsbehörde NHTSA eröffnete Untersuchung des Unfalls. Doch bereits im zweiten Satz ging der Spin in Richtung Statistik:

> This is the first known fatality in just over 130 million miles where Autopilot was activated. Among all vehicles in the US, there is a fatality every 94 million miles. Worldwide, there is a fatality approximately every 60 million miles.

Mit gutem Willen übersetzt: Die Fahrzeuge mit Autopilot sind 38 % sicherer als der US-Durchschnitt. Diese Statistiken sind jedoch **irreführend**: Sie vergleichen die Gesamtheit der Fahrzeuge der USA (ca. [254 Millionen](https://en.wikipedia.org/wiki/Passenger_vehicles_in_the_United_States#Total_number_of_vehicles), von denn über 60 % älter als sieben Jahre sind) mit jenen ca. 71.000 verkauften Model S (Listenpreis: 60.000–120.000 Dollar), bzw. einer noch kleineren Zahl der Fahrzeuge, für die der 2.500 Dollar Aufpreis für den Autopilot gezahlt wurde. Selbstverständlich sind neue Oberklassefahrzeuge mit einem Gewicht von zwei Tonnen und vollgestopft mit Fahrassitsenzsystemen überdurchschnittlich sicher! Der weltweite Vergleich bringt da zwar noch eindrucksvollere Zahlen (Mehr als doppelt so sicher!!!), jedoch auf Kosten eines Vergleichs der Flotte von Ländern mit weit geringeren Sicherheitsstandards.

Wenn man die Aussage böswilliger interpretiert, könnte man auch sagen: Ein solcher Unfall ist seit 36 Mio. Meilen **überfällig**. Und natürlich hätte Tesla davon ausgehen müssen, dass irgendwann die Situation eintritt, in der ein Fahrer sich entgegen der Warnungen verhält und einen Unfall provoziert. In einer solchen Situation (und mit ausreichend zeitlichem Abstand) ist die Reaktion jedoch um so enttäuschender, wenn die Argumentation so nichtssagend ist.

> It is important to note that Tesla disables Autopilot by default and requires explicit acknowledgement that the system is new technology and still in a public beta phase before it can be enabled.

Juristisch sicherlich alles korrekt. Allerdings suggeriert "Autopilot" ein Bild eines selbstfahrenden Autos, "New Technology" ist das hart erarbeitete Image von Tesla als disruptiver Player im verkrusteten Automobilmarkt, und "Beta-Phase" ist lange keine Warnung für potenziell fehlerhafte Software, sondern in Zeiten Smartphones und Web-Apps der Regelzustand und ein Synonym für "neu".

> When drivers activate Autopilot, the acknowledgment box explains, among other things, that Autopilot “is an assist feature that requires you to keep your hands on the steering wheel at all times,” and that “you need to maintain control and responsibility for your vehicle” while using it. Additionally, every time that Autopilot is engaged, the car reminds the driver to “Always keep your hands on the wheel. Be prepared to take over at any time.”

Wie ich oben schon schrieb, müsste Tesla wissen, dass sich Fahrer nicht an diese Vorgaben halten, und das Wording des "Autopilot" und "Autosteer" eben gerade eine Funktionalität beschreiben, die die eine Aufgabe des Fahrers übernehmen.

> The system also makes frequent checks to ensure that the driver’s hands remain on the wheel and provides visual and audible alerts if hands-on is not detected. It then gradually slows down the car until hands-on is detected again.

Ich habe unterschiedliche Berichte gelesen bzw. Videos gesehen, in denen über Minuten hinweg die Hände vom Lenkrad genommen werden können, ohne dass gewarnt wurde. Da Tesla die Software ständig aktualisiert, kann dies jedoch inzwischen behoben worden sein.

Es ändert jedoch nichts daran, dass Tesla sich nicht der Verantwortung für den ungelösten Konflikt zwischen einer wachsenden Entmündigung des Fahrers bei gleichzeitig verlangter Aufmerksamkeit stellt.

#### Nachtrag 13.07.2016

Bei einem [weiteren Unfall](http://www.ktvq.com/story/32427989/tesla-confirms-autopilot-crash-in-montana) mit dem Autopilot (mittlerweile der dritte) auf einem zweispurigen Highway wird klar, dass Tesla offenbar noch immer nicht durchsetzt, dass der Fahrer die Hände am Lenkrad lässt (Hervorhebung durch mich):

> Tesla confirmed that the data it has from the car shows it was in Autopilot mode, and that the driver likely did not have his hands on the wheel.
> 
> “No force was detected on the steering wheel **for over two minutes** after autosteer was engaged,” said Tesla, which added that it can detect even a very small amount of force, such as one hand resting on the wheel.

## Fazit

Ich hätte mir ein stärkeres Statement von Tesla gewünscht, in denen sie auf die neuen **Herausforderungen** auf dem Weg zu teilautonomen Fahren eingegangen wären und die bestehenden Probleme nicht hinter "ist noch Beta", "wird noch besser" und "ist statistisch überdurchschnittlich sicher" versteckt hätten. Sie müssen nicht die Fahrer in Schutz nehmen, die sich nicht an die Regeln des Systems halten. Doch sie müssen für ihre Software Verantwortung übernehmen und sich fragen, ob sie genug getan haben, um die Regeln des Systems durchzusetzen.

Carl Adam Petri sagte einmal:

> Disliked things must not be forbidden: they have to be impossible.

Ich glaube natürlich daran, dass der technische Fortschritt mit besseren Assistenzsystemen zu weniger Unfällen und Verkehrstoten führen wird. Sicherlich werden diese Systeme monotone und gleichförmige Situationen weit besser als menschliche Fahrer meistern können. Und auch in Extremsituationen haben diese Systeme das Potenzial, durch kürzere Reaktion auf weit mehr Daten bessere Entscheidungen zu treffen. Allerdings kann diese Entwicklung nicht am Fahrer vorbei passieren. Solange der Fahrer nicht aus der Verantwortung entlassen werden kann, muss er immer mehr bei der Entwicklung beachtet werden, sonst verantwortet er am Ende nur noch die beinahen oder tatsächlichen Unfälle, während der Autopilot für das Meistern der monotonen Arbeit eine weiße Weste vorweisen kann.

Außerdem muss ein **andere Haltung** gegenüber einer solchen Software gefunden werden. Wenn zehntausende Fahrzeuge den Fahrer quasi zum aufmerksamen Zuschauer bevormunden, sind Konzepte wie "Beta" oder "constantly improving" fehl am Platz. Während jeder Fehler eines Fahrers individuell ist, ist ein Fehler in der Software zehntausendfach verbreitet und jederzeit reproduzierbar.

#### Nachtrag 11.07.2015

In einem [Tweet](https://twitter.com/elonmusk/status/752190828798353408) kommentiert Teslas CEO Elon Musk sein Verständnis von **Beta-Software**

> Misunderstanding of what “beta” means to Tesla for Autopilot: any system w less than 1B miles of real world driving

Ein System ist also "Beta", wenn es weniger als 1 Milliarde Meilen in der "echten Welt" getestet wurde. Dass dies bei Tesla aber offenbar nicht (nur) ausgebildete Testfahrer, sondern (auch) die Kunden übernehmen, bleibt unerwähnt.

Ein kleiner [Rant von Sascha Pallenberg](http://www.mobilegeeks.de/news/kraftfahrt-bundesamt-tesla-autopilot/) bringt dies gut auf den Punkt:

{% youtube k4x8dtiANWE %}


* * *

*Disclaimer: Ich arbeite bei einer Tochter des Volkswagen-Konzerns. Dieser Blogpost drückt ausschließlich meine eigene Meinung aus.*

*[ACC]: Adaptive Cruise Control
*[AGB]: Allgemeine Geschäftsbedingungen
*[CEO]: Chief Executive Officer
*[NHTSA]: National Highway Traffic Safety Administration
